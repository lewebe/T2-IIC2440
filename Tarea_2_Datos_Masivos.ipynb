{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6MzWbD_s6Bs"
      },
      "source": [
        "Tarea 2 Procesamiento de Datos Masivos\n",
        "Departamento de Ciencia de la Computación\n",
        "\n",
        "Universidad Catolica de Chile\n",
        "\n",
        "Profesor: Juan Reutter\n",
        "\n",
        "Alumnos: Juan Vicuña y Clemente Cambara"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Algoritmo Page Rank"
      ],
      "metadata": {
        "id": "PNH6VWGK2tK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPnwn3detnMl",
        "outputId": "08c206a0-fa53-4e8b-9ec5-765d00b4a1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones necesarias"
      ],
      "metadata": {
        "id": "peAMMr5325U3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "BKwGxPjatGay",
        "outputId": "5d1b24cb-5c4e-47d1-a833-b7182a84bf60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5d85517e5d03>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PageRank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Función para preparar el RDD inicial con los Page Ranks iniciales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitialize_page_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PageRank, master=local) created by __init__ at <ipython-input-43-5d85517e5d03>:1 "
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"PageRank\")\n",
        "\n",
        "# Función para preparar el RDD inicial con los Page Ranks iniciales\n",
        "def initialize_page_ranks(nodes):\n",
        "    num_nodes = nodes.count()\n",
        "    initial_rank = 1.0 / num_nodes\n",
        "    return nodes.map(lambda node: (node, initial_rank))\n",
        "\n",
        "# Función para preparar los mensajes que cada nodo enviará\n",
        "def prepare_messages(node, neighbors, rank):\n",
        "    num_neighbors = len(neighbors)\n",
        "    messages = []\n",
        "    if num_neighbors > 0:\n",
        "        rank_per_neighbor = rank / num_neighbors\n",
        "        for neighbor in neighbors:\n",
        "            messages.append((neighbor, rank_per_neighbor))\n",
        "    return messages\n",
        "\n",
        "# Función para intercambiar mensajes entre nodos y realizar el merge de los mensajes recibidos\n",
        "def send_messages(node, neighbors, rank):\n",
        "    return prepare_messages(node, neighbors, rank)\n",
        "\n",
        "# Función para actualizar el valor de Page Rank para cada nodo considerando el damping factor\n",
        "def update_page_rank(node, old_rank, damping_factor, messages):\n",
        "    print(\"Update\")\n",
        "    new_rank = (1 - damping_factor)//num_nodes + damping_factor * messages\n",
        "    return (node, new_rank)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación"
      ],
      "metadata": {
        "id": "0aXUP5843AXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu23fR95uPeF"
      },
      "outputs": [],
      "source": [
        "def PageRanks(page_ranks, max_iterations, min_diff):\n",
        "  for iteration in range(max_iterations):\n",
        "      print(f\"Iteración numero: {iteration}\")\n",
        "      # Realizar el paso de envío de mensajes entre nodos\n",
        "      messages = links.join(page_ranks).flatMap(lambda x: send_messages(x[0], x[1][0], x[1][1]))\n",
        "\n",
        "\n",
        "      # Realizar el paso de merge de los mensajes recibidos por cada nodo\n",
        "      merged_messages = messages.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "      # Actualizar el valor de PageRank para cada nodo considerando el damping factor\n",
        "      page_ranks = page_ranks.join(merged_messages).map(lambda x: update_page_rank(x[0], x[1][0], 0.85, x[1][1]))\n",
        "\n",
        "      # Calcular la diferencia entre iteraciones\n",
        "      diff = page_ranks.join(merged_messages).map(lambda x: abs(x[1][0] - x[1][1])).sum()\n",
        "\n",
        "      print(f'Diferencia igual a {diff}')\n",
        "      print(\"---------------------------------------------------------------------------\")\n",
        "\n",
        "      # Verificar si la diferencia es menor que el umbral mínimo\n",
        "      if diff < min_diff:\n",
        "          break\n",
        "\n",
        "  return page_ranks.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos los datos"
      ],
      "metadata": {
        "id": "XkG34TUq21nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos de nodos y enlaces desde un archivo (suponiendo formato: nodo1,nodo2)\n",
        "links_data = sc.textFile(\"links.txt\")\n",
        "# Crear el RDD de enlaces\n",
        "links = links_data.map(lambda x: tuple(x.split(\",\")))\n",
        "# Crear el RDD de nodos\n",
        "nodes = links.flatMap(lambda x: x).distinct()\n",
        "\n",
        "num_nodes = nodes.count()"
      ],
      "metadata": {
        "id": "KCW3ETYs1Y2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiperparámetros"
      ],
      "metadata": {
        "id": "9ueqVwA84KPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el número máximo de iteraciones y la diferencia mínima entre iteraciones\n",
        "max_iterations = 5\n",
        "min_diff = 0.0001"
      ],
      "metadata": {
        "id": "himWIPF71nIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucción"
      ],
      "metadata": {
        "id": "XmaJhwSn4Ma1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar los Page Ranks\n",
        "page_ranks = initialize_page_ranks(nodes)\n",
        "\n",
        "final_ranks = PageRanks(page_ranks, max_iterations, min_diff)\n",
        "for node, rank in final_ranks:\n",
        "    print(\"Node:\", node, \"Rank:\", rank)\n",
        "\n",
        "# Detener el contexto de Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMFN6Cwd1vWY",
        "outputId": "d025fc82-7405-4c98-fbd7-fb4a266040d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración numero: 0\n",
            "Diferencia igual a 0.3750000000000001\n",
            "---------------------------------------------------------------------------\n",
            "Iteración numero: 1\n",
            "Diferencia igual a 0.765\n",
            "---------------------------------------------------------------------------\n",
            "Iteración numero: 2\n",
            "Diferencia igual a 1.5714375000000005\n",
            "---------------------------------------------------------------------------\n",
            "Iteración numero: 3\n",
            "Diferencia igual a 3.2241562499999987\n",
            "---------------------------------------------------------------------------\n",
            "Iteración numero: 4\n",
            "Diferencia igual a 6.61642921875\n",
            "---------------------------------------------------------------------------\n",
            "Node: A Rank: 10.981706484375\n",
            "Node: D Rank: 4.5479794531249995\n",
            "Node: C Rank: 10.981706484375\n",
            "Node: B Rank: 10.981706484375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Algoritmo Single Source Shortest Path"
      ],
      "metadata": {
        "id": "BLkvuFQP4jll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones necesarias"
      ],
      "metadata": {
        "id": "V1WLDZ4cDYaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el contexto de Spark\n",
        "sc = SparkContext(\"local\", \"SSSP\")\n",
        "\n",
        "\n",
        "# Función para preparar los mensajes que cada nodo va a enviar\n",
        "def prepare_messages(source_cost, destination, cost):\n",
        "    return [(destination, source_cost + cost)]\n",
        "\n",
        "# Función para realizar el merge de los mensajes recibidos por cada nodo\n",
        "def merge_messages(a, b):\n",
        "    return min(a, b)\n",
        "\n",
        "# Función para actualizar los costos acumulados de los nodos\n",
        "def update_node(node, current_cost, message_cost):\n",
        "    new_cost = min(current_cost, message_cost)\n",
        "    return (node, new_cost)"
      ],
      "metadata": {
        "id": "H_uiG_6c5yqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "s2-I8u-t8GKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación"
      ],
      "metadata": {
        "id": "ltG5mZ78DbqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SSSP(edges_rdd, node_costs):\n",
        "\n",
        "  # Iterar el algoritmo Single Source Shortest Path\n",
        "  for iteration in range(max_iterations):\n",
        "      print(f\"Iteracion número: {iteration}\")\n",
        "\n",
        "      # Realizar el paso de envío de mensajes entre nodos\n",
        "      edges_costs_join = edges_rdd.map(lambda x: (x[0], (x[1], x[2]))).join(node_costs).map(lambda x: (x[0], (x[1][0][0], x[1][0][1], x[1][1])))\n",
        "      messages = edges_costs_join.flatMap(lambda x: prepare_messages(x[1][2], x[1][0], x[1][1]))\n",
        "\n",
        "      # Realizar el paso de merge de los mensajes recibidos por cada nodo\n",
        "      merged_messages = messages.reduceByKey(merge_messages)\n",
        "\n",
        "      # Actualizar los costos acumulados de los nodos\n",
        "      node_costs = node_costs.join(merged_messages).map(lambda x: update_node(x[0], x[1][0], x[1][1]))\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "  return node_costs.collect()\n"
      ],
      "metadata": {
        "id": "D1qXFiHx5dMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiperparámetros"
      ],
      "metadata": {
        "id": "diZXe5F86gKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iterations = 2\n",
        "initial_node = 1"
      ],
      "metadata": {
        "id": "fAjtXJM76h8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creamos un grafo ejemplo"
      ],
      "metadata": {
        "id": "v1htxhjy6mqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges = [(1, 2, 10), (2, 3, 3), (2, 4, 24), (3, 2, 1)]\n",
        "\n",
        "# Crear el RDD de aristas\n",
        "edges_rdd = sc.parallelize(edges)\n",
        "\n",
        "# Obtener todos los nodos\n",
        "nodes = edges_rdd.flatMap(lambda x: (x[0], x[1])).distinct()\n",
        "\n",
        "# Inicializar los costos acumulados de los nodos\n",
        "node_costs = nodes.map(lambda x: (x, float(\"inf\")))\n",
        "node_costs = node_costs.map(lambda x: (initial_node, 0.0) if x[0] == initial_node else x)\n"
      ],
      "metadata": {
        "id": "n922OEEj6H5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucción"
      ],
      "metadata": {
        "id": "GkKry-9sDg1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_costs = SSSP(edges_rdd, node_costs)\n",
        "\n",
        "for node, cost in final_costs:\n",
        "    print(\"Node:\", node, \"Cost:\", cost)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbAVmK0p6Qje",
        "outputId": "96e2a2a3-2f55-41de-8951-c3c72972c638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteracion número: 0\n",
            "[(2, (3, 3, inf)), (2, (4, 24, inf)), (1, (2, 10, 0.0)), (3, (2, 1, inf))]\n",
            "[(3, inf), (4, inf), (2, 10.0), (2, inf)]\n",
            "Iteracion número: 1\n",
            "[(2, (3, 3, 10.0)), (2, (4, 24, 10.0)), (3, (2, 1, inf))]\n",
            "[(3, 13.0), (4, 34.0), (2, inf)]\n",
            "Node: 2 Cost: 10.0\n",
            "Node: 3 Cost: 13.0\n",
            "Node: 4 Cost: 34.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}